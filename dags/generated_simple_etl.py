"""
Auto-generated DAG from simple_etl.py
Generated: 2026-01-25 18:03:57
Total tasks: 5
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import logging
import pandas as pd
import numpy as np
import json
import os
from pathlib import Path

# Default arguments for the DAG
default_args = {
    "owner": "data_engineer",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(seconds=5),
}

# === Ð’Ð¡Ð¢Ð ÐžÐ•ÐÐÐ«Ð• Ð¤Ð£ÐÐšÐ¦Ð˜Ð˜ Ð˜Ð— Ð˜Ð¡Ð¥ÐžÐ”ÐÐžÐ“Ðž Ð¤ÐÐ™Ð›Ð ===

# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ: generate_sample_data
def generate_sample_data(**kwargs):
    """Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ, Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð´Ð»Ñ Airflow"""
    logger = logging.getLogger(__name__)
    logger.info("Starting generate_sample_data")
    try:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        logger.info("Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ...")

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ DataFrame Ñ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
        data = {
            'user_id': range(1, 101),
            'name': [f'User_{i}' for i in range(1, 101)],
            'age': np.random.randint(18, 65, 100),
            'city': np.random.choice(['Moscow', 'SPb', 'Kazan', 'Novosibirsk'], 100),
            'signup_date': [datetime.now() - timedelta(days=np.random.randint(0, 365))
                            for _ in range(100)],
            'revenue': np.random.uniform(10, 1000, 100).round(2)
        }

        df = pd.DataFrame(data)
        logger.info(f"Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ {len(df)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
        return df
    except Exception as e:
        logger.error(f"Error in generate_sample_data: {e}")
        raise
    finally:
        logger.info("Completed generate_sample_data")

# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ: clean_data
def clean_data(**kwargs):
    """Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ, Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð´Ð»Ñ Airflow"""
    logger = logging.getLogger(__name__)
    logger.info("Starting clean_data")
    try:
        """ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        logger.info("ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ…...")

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        df = generate_sample_data()

        # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹
        df = df.drop_duplicates(subset=['user_id'])

        # Ð—Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¸
        df['age'] = df['age'].fillna(df['age'].median())

        # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²Ñ‹Ð±Ñ€Ð¾ÑÑ‹ Ð² revenue
        q1 = df['revenue'].quantile(0.25)
        q3 = df['revenue'].quantile(0.75)
        iqr = q3 - q1
        df = df[(df['revenue'] >= q1 - 1.5 * iqr) & (df['revenue'] <= q3 + 1.5 * iqr)]

        logger.info(f"ÐŸÐ¾ÑÐ»Ðµ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸ Ð¾ÑÑ‚Ð°Ð»Ð¾ÑÑŒ {len(df)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
        return df
    except Exception as e:
        logger.error(f"Error in clean_data: {e}")
        raise
    finally:
        logger.info("Completed clean_data")

# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ: calculate_metrics
def calculate_metrics(**kwargs):
    """Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ, Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð´Ð»Ñ Airflow"""
    logger = logging.getLogger(__name__)
    logger.info("Starting calculate_metrics")
    try:
        """Ð Ð°ÑÑ‡ÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸Ðº"""
        logger.info("Ð Ð°ÑÑ‡ÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸Ðº...")

        df = clean_data()

        metrics = {
            'total_users': len(df),
            'avg_age': df['age'].mean().round(2),
            'total_revenue': df['revenue'].sum().round(2),
            'avg_revenue_per_user': (df['revenue'].sum() / len(df)).round(2),
            'users_by_city': df['city'].value_counts().to_dict(),
            'top_10_users': df.nlargest(10, 'revenue')[['user_id', 'name', 'revenue']].to_dict('records')
        }

        logger.info(f"ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸: {metrics}")
        return metrics
    except Exception as e:
        logger.error(f"Error in calculate_metrics: {e}")
        raise
    finally:
        logger.info("Completed calculate_metrics")

# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ: save_to_json
def save_to_json(**kwargs):
    """Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ, Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð´Ð»Ñ Airflow"""
    logger = logging.getLogger(__name__)
    logger.info("Starting save_to_json")
    try:
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð² JSON"""
        logger.info("Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² JSON...")

        metrics = calculate_metrics()

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
        import json
        from pathlib import Path

        output_dir = Path("/tmp/airflow_output")
        output_dir.mkdir(exist_ok=True)

        filename = output_dir / f"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(filename, 'w') as f:
            json.dump(metrics, f, indent=2, default=str)

        logger.info(f"ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² {filename}")
        return str(filename)
    except Exception as e:
        logger.error(f"Error in save_to_json: {e}")
        raise
    finally:
        logger.info("Completed save_to_json")

# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ: send_notification
def send_notification(**kwargs):
    """Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ, Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð´Ð»Ñ Airflow"""
    logger = logging.getLogger(__name__)
    logger.info("Starting send_notification")
    try:
        """ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ð¸"""
        logger.info("ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ...")

        # Ð—Ð´ÐµÑÑŒ Ð¼Ð¾Ð³Ð»Ð° Ð±Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Ñ Slack/Telegram/Email
        filename = save_to_json()

        message = f"""
        âœ… ETL Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½!
        ðŸ“Š Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð²: {filename}
        ðŸ•’ Ð’Ñ€ÐµÐ¼Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ: {datetime.now()}
        """

        logger.info(message)

        # Ð”Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÐ¼ Ð² Ð»Ð¾Ð³
        notification_file = Path("/tmp/airflow_output/notifications.log")
        with open(notification_file, 'a') as f:
            f.write(f"{datetime.now()}: {message}\n")

        return True
    except Exception as e:
        logger.error(f"Error in send_notification: {e}")
        raise
    finally:
        logger.info("Completed send_notification")

with DAG(
    dag_id="generated_simple_etl",
    default_args=default_args,
    start_date=datetime(2026, 1, 25),
    schedule_interval="@hourly",
    catchup=False,
    tags=["etl", "simple", "data"]
) as dag:

    # Python Ð·Ð°Ð´Ð°Ñ‡Ð¸:
    execute_clean_data = PythonOperator(
        task_id="execute_clean_data",
        python_callable=clean_data,
        doc_md="""ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…""",
    )