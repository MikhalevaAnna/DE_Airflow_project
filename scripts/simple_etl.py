# DAG_SCHEDULE_INTERVAL: @hourly
# DAG_TAGS: ['etl', 'simple', 'data']
# DAG_OWNER: data_engineer
# DAG_RETRIES: 2
# DAG_RETRY_DELAY: 5
from pathlib import Path

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)


def generate_sample_data():
    """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
    logger.info("Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ...")

    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ DataFrame Ñ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸
    data = {
        'user_id': range(1, 101),
        'name': [f'User_{i}' for i in range(1, 101)],
        'age': np.random.randint(18, 65, 100),
        'city': np.random.choice(['Moscow', 'SPb', 'Kazan', 'Novosibirsk'], 100),
        'signup_date': [datetime.now() - timedelta(days=np.random.randint(0, 365))
                        for _ in range(100)],
        'revenue': np.random.uniform(10, 1000, 100).round(2)
    }

    df = pd.DataFrame(data)
    logger.info(f"Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ {len(df)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
    return df


def clean_data():
    """ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
    logger.info("ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ…...")

    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
    df = generate_sample_data()

    # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹
    df = df.drop_duplicates(subset=['user_id'])

    # Ð—Ð°Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¸
    df['age'] = df['age'].fillna(df['age'].median())

    # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²Ñ‹Ð±Ñ€Ð¾ÑÑ‹ Ð² revenue
    q1 = df['revenue'].quantile(0.25)
    q3 = df['revenue'].quantile(0.75)
    iqr = q3 - q1
    df = df[(df['revenue'] >= q1 - 1.5 * iqr) & (df['revenue'] <= q3 + 1.5 * iqr)]

    logger.info(f"ÐŸÐ¾ÑÐ»Ðµ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸ Ð¾ÑÑ‚Ð°Ð»Ð¾ÑÑŒ {len(df)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
    return df


def calculate_metrics():
    """Ð Ð°ÑÑ‡ÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸Ðº"""
    logger.info("Ð Ð°ÑÑ‡ÐµÑ‚ Ð¼ÐµÑ‚Ñ€Ð¸Ðº...")

    df = clean_data()

    metrics = {
        'total_users': len(df),
        'avg_age': df['age'].mean().round(2),
        'total_revenue': df['revenue'].sum().round(2),
        'avg_revenue_per_user': (df['revenue'].sum() / len(df)).round(2),
        'users_by_city': df['city'].value_counts().to_dict(),
        'top_10_users': df.nlargest(10, 'revenue')[['user_id', 'name', 'revenue']].to_dict('records')
    }

    logger.info(f"ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸: {metrics}")
    return metrics


def save_to_json():
    """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² Ð² JSON"""
    logger.info("Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² JSON...")

    metrics = calculate_metrics()

    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
    import json
    from pathlib import Path

    output_dir = Path("/tmp/airflow_output")
    output_dir.mkdir(exist_ok=True)

    filename = output_dir / f"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    with open(filename, 'w') as f:
        json.dump(metrics, f, indent=2, default=str)

    logger.info(f"ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð² {filename}")
    return str(filename)


def send_notification():
    """ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ð¸"""
    logger.info("ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ...")

    # Ð—Ð´ÐµÑÑŒ Ð¼Ð¾Ð³Ð»Ð° Ð±Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Ñ Slack/Telegram/Email
    filename = save_to_json()

    message = f"""
    âœ… ETL Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½!
    ðŸ“Š Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð²: {filename}
    ðŸ•’ Ð’Ñ€ÐµÐ¼Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ: {datetime.now()}
    """

    logger.info(message)

    # Ð”Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÐ¼ Ð² Ð»Ð¾Ð³
    notification_file = Path("/tmp/airflow_output/notifications.log")
    with open(notification_file, 'a') as f:
        f.write(f"{datetime.now()}: {message}\n")

    return True